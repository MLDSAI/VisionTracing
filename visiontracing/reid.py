# -*- coding: utf-8 -*-
'''subject-reid-prototype.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qcAIyqFYT7IMSPa56cy0DnrGGGC61KQH

# Subject Re-ID Prototype

### Re-ID Information 

### Workflow
1. Import video data
2. Use Detectron to get bounding box predictions and frames
3. [Not Implemented] Run NMS to consolidate similar bounding boxes
4. Run object tracking using simple OpenCV-based centroid tracker
5. [Not Implemented] Filter out irrelevant tracklets
6. Save bounding boxes to dataset on disk
---
7. Run subject re-identification
8. Match query and gallery results between 2 videos using greedy bipartite matching
9. Produce demo video with color-coded IDs

### Notes
- Currently runs re-ID successfully for two videos (one query video, one gallery video).
- Skip steps 1-6 if you already have detection and track data.
- Ignore code sections labelled [NOT USED] - these are experiments or additional code (other tracking methods, for example) that are not necessary to the base re-ID functionality.

### Imports and Setup
'''

# INSTALLATIONS

# install dependencies: (use cu101 because colab has CUDA 10.1)
'''
!pip install -U torch==1.6 torchvision==0.7 -f https://download.pytorch.org/whl/cu101/torch_stable.html
!pip install cython pyyaml==5.1 cython_bbox
!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'
!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/index.html
!pip install pretrainedmodels
'''

# IMPORTS

import os
import sys
import time
from tqdm.notebook import tqdm
from pprint import pprint, pformat
import random
import statistics
import requests
from collections import OrderedDict

import numpy as np
import cv2
import torch, torchvision
from scipy.spatial import distance as dist
from scipy.spatial.distance import pdist, squareform

import detectron2
from detectron2.utils.logger import setup_logger
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.data import MetadataCatalog
from detectron2.utils import visualizer
from detectron2.utils.visualizer import ColorMode
from detectron2.utils.video_visualizer import VideoVisualizer

import imageio
from skimage.color import rgba2rgb
from cython_bbox import bbox_overlaps

from google.colab import drive, files
from google.colab.patches import cv2_imshow

'''### Upload Input Video Files'''

# get input video files by uploading or from google drive
# if uploading videos, upload at least two videos - uploading these may take a while

input_video_files = []
UPLOAD_FILES = False

if UPLOAD_FILES:
  uploaded = files.upload()
  for fn in uploaded.keys():
    print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn]))
    )

  for video_filename in list(uploaded.keys()):
    print('video_filename:', video_filename)
    input_video_files.append(video_filename)
  
  sample_video_dir = "/content/"
else:
  # mount google drive
  drive.mount('/content/drive')
  
  # change this directory to gdrive directory with input videos
  sample_video_dir = "/content/drive/My Drive/reid-sample-videos"
  for video in os.listdir(sample_video_dir):
    input_video_files.append(os.path.join(sample_video_dir, video))

# view all input videos
for i, video in enumerate(input_video_files):
  print(i, video)

# select two input videos to use for re-ID using video index in input_video_files list

# CHANGE THESE VALUES TO CHANGE INPUT VIDEOS
QUERY_VIDEO_IDX = 9
GALLERY_VIDEO_IDX = 10

curr_query_video = input_video_files[QUERY_VIDEO_IDX]
curr_gallery_video = input_video_files[GALLERY_VIDEO_IDX]

'''### Preparing Dataset

#### Detectron Configs
'''

# get detectron configs, default model, metadata
SCORE_THRESHOLD = 0.9
cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml"))
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = SCORE_THRESHOLD 
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml")
metadata = MetadataCatalog.get(
  cfg.DATASETS.TEST[0] if len(cfg.DATASETS.TEST) else "__unused"
)
cpu_device = torch.device('cpu')
instance_mode = ColorMode.IMAGE

# initialize detectron predictor
predictor = DefaultPredictor(cfg)

'''#### Multi Object Tracking (Centroid Tracking)

Code from https://www.pyimagesearch.com/2018/07/23/simple-object-tracking-with-opencv/
'''

class CentroidTrackerModified():
	def __init__(self, maxDisappeared=50):
		# initialize the next unique object ID along with two ordered
		# dictionaries used to keep track of mapping a given object
		# ID to its centroid and number of consecutive frames it has
		# been marked as "disappeared", respectively
		self.nextObjectID = 0
		self.objects = OrderedDict()
		self.objects_to_last_bbox = OrderedDict()
		self.disappeared = OrderedDict()

		# store the number of maximum consecutive frames a given
		# object is allowed to be marked as "disappeared" until we
		# need to deregister the object from tracking
		self.maxDisappeared = maxDisappeared

	def register(self, centroid, bbox):
		# when registering an object we use the next available object
		# ID to store the centroid
		self.objects[self.nextObjectID] = centroid
		self.disappeared[self.nextObjectID] = 0
		self.objects_to_last_bbox[self.nextObjectID] = bbox
		self.nextObjectID += 1

	def deregister(self, objectID):
		# to deregister an object ID we delete the object ID from
		# both of our respective dictionaries
		del self.objects[objectID]
		del self.disappeared[objectID]
		del self.objects_to_last_bbox[objectID]

	def update(self, rects):
		# check to see if the list of input bounding box rectangles is empty
		if len(rects) == 0:
			# loop over any existing tracked objects and mark them as disappeared
			for objectID in list(self.disappeared.keys()):
				self.disappeared[objectID] += 1

				# if we have reached a maximum number of consecutive
				# frames where a given object has been marked as
				# missing, deregister it
				if self.disappeared[objectID] > self.maxDisappeared:
					self.deregister(objectID)

			# return early as there are no centroids or tracking info
			# to update
			return (self.objects, self.objects_to_last_bbox)

		# initialize an array of input centroids for the current frame
		inputCentroids = np.zeros((len(rects), 2), dtype="int")

		# loop over the bounding box rectangles
		for (i, (startX, startY, endX, endY)) in enumerate(rects):
			# use the bounding box coordinates to derive the centroid
			cX = int((startX + endX) / 2.0)
			cY = int((startY + endY) / 2.0)
			inputCentroids[i] = (cX, cY)

		# if we are currently not tracking any objects take the input
		# centroids and register each of them
		if len(self.objects) == 0:
			# for i in range(0, len(inputCentroids)):
			# 	self.register(inputCentroids[i])

			for inputCentroid, bbox in zip(inputCentroids, rects):
				self.register(inputCentroid, bbox)

		# otherwise, are are currently tracking objects so we need to
		# try to match the input centroids to existing object
		# centroids
		else:
			# grab the set of object IDs and corresponding centroids
			objectIDs = list(self.objects.keys())
			objectCentroids = list(self.objects.values())

			# compute the distance between each pair of object
			# centroids and input centroids, respectively -- our
			# goal will be to match an input centroid to an existing
			# object centroid
			D = dist.cdist(np.array(objectCentroids), inputCentroids)

			# in order to perform this matching we must (1) find the
			# smallest value in each row and then (2) sort the row
			# indexes based on their minimum values so that the row
			# with the smallest value as at the *front* of the index
			# list
			rows = D.min(axis=1).argsort()

			# next, we perform a similar process on the columns by
			# finding the smallest value in each column and then
			# sorting using the previously computed row index list
			cols = D.argmin(axis=1)[rows]

			# in order to determine if we need to update, register,
			# or deregister an object we need to keep track of which
			# of the rows and column indexes we have already examined
			usedRows = set()
			usedCols = set()

			# loop over the combination of the (row, column) index
			# tuples
			for (row, col) in zip(rows, cols):
				# if we have already examined either the row or
				# column value before, ignore it
				# val
				if row in usedRows or col in usedCols:
					continue

				# otherwise, grab the object ID for the current row,
				# set its new centroid, and reset the disappeared counter
				objectID = objectIDs[row]
				self.objects[objectID] = inputCentroids[col]
				self.disappeared[objectID] = 0
				self.objects_to_last_bbox[objectID] = rects[col]

				# indicate that we have examined each of the row and
				# column indexes, respectively

				usedRows.add(row)
				usedCols.add(col)

			# compute both the row and column index we have NOT yet examined

			unusedRows = set(range(0, D.shape[0])).difference(usedRows)
			unusedCols = set(range(0, D.shape[1])).difference(usedCols)

			# in the event that the number of object centroids is
			# equal or greater than the number of input centroids
			# we need to check and see if some of these objects have
			# potentially disappeared
			if D.shape[0] >= D.shape[1]:
				# loop over the unused row indexes
				for row in unusedRows:
					# grab the object ID for the corresponding row
					# index and increment the disappeared counter
					objectID = objectIDs[row]
					self.disappeared[objectID] += 1

					# check to see if the number of consecutive
					# frames the object has been marked "disappeared"
					# for warrants deregistering the object
					if self.disappeared[objectID] > self.maxDisappeared:
						self.deregister(objectID)

			# otherwise, if the number of input centroids is greater
			# than the number of existing object centroids we need to
			# register each new input centroid as a trackable object
			else:
				for col in unusedCols:
					self.register(inputCentroids[col], rects[col])

		# return the set of trackable objects
		return (self.objects, self.objects_to_last_bbox)

def get_track_information(video_input, max_frames=1000, stride=1, GET_VIDEO_OUTPUT=False):
  '''
  Returns track information after running keypoint detection and simple centroid tracking.

  video_input -- full absolute path of input video
  max_frames -- upper limit of frames to process
  stride -- run detection and tracking at every x # of frames
  GET_VIDEO_OUTPUT -- set true if video output is required to analyze effectiveness of detectron and tracking
  '''

  # configure input video
  video_filename = video_input[:-4]
  vs = cv2.VideoCapture(video_input)
  width = int(vs.get(cv2.CAP_PROP_FRAME_WIDTH))
  height = int(vs.get(cv2.CAP_PROP_FRAME_HEIGHT))
  frames_per_second = vs.get(cv2.CAP_PROP_FPS)
  num_frames = int(vs.get(cv2.CAP_PROP_FRAME_COUNT))
  print(video_input)
  print(num_frames)

  # initialize centroid tracker and settings
  ct = CentroidTrackerModified()
  frame_counter = 0
  num_frames_to_output = max_frames
  pbar = tqdm(total = num_frames_to_output)

  if GET_VIDEO_OUTPUT:
    output_fname = f'{vid}-output.mp4'
    output_file = cv2.VideoWriter(
      filename=output_fname,
      fourcc=cv2.VideoWriter_fourcc(*'mp4v'),
      fps=float(frames_per_second),
      frameSize=(width, height),
      isColor=True,
    )

  # video data needed for re-id matching
  object_id_to_images = {}
  object_id_to_bbox = {}
  frame_to_bbox = {}

  while True:
    frame = vs.read()[1]

    if frame is None or frame_counter > num_frames_to_output:
      break

    # store images only every few frames
    if frame_counter % stride == 0:
      outputs = predictor(frame)
      predictions = outputs["instances"].to(cpu_device)
      predictions.remove("pred_keypoints")
      tensors = predictions.pred_boxes.tensor.numpy()

      rects = []
      for idx, tensor in enumerate(tensors):
        box = [int(v) for v in tensor]
        rects.append(box)

      # update centroid tracker using bounding boxes
      (object_to_centroids, object_to_bboxes) = ct.update(rects)

      # draw over objects with information, loop over tracked objects
      for (objectID, centroid) in object_to_centroids.items():
        # save track information
        (startX, startY, endX, endY) = box = object_to_bboxes[objectID]
        imCrop = frame[startY:endY, startX:endX]

        if GET_VIDEO_OUTPUT:
          text = "ID {}".format(objectID)
          cv2.rectangle(frame, (startX, startY), 
                        (endX, endY), (0, 255, 0), 1)
          cv2.putText(frame, text, (centroid[0] - 10, centroid[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

        # save object tracks and other information
        if objectID in object_id_to_images:
          object_id_to_images[objectID].append(imCrop)
        else:
          object_id_to_images[objectID] = [imCrop]

        if objectID in object_id_to_bbox:
          object_id_to_bbox[objectID].append(box)
        else:
          object_id_to_bbox[objectID] = [box]

        if frame_counter in frame_to_bbox:
          frame_to_bbox[frame_counter].append(box)
        else:
          frame_to_bbox[frame_counter] = [tuple(box)]

    if GET_VIDEO_OUTPUT:
      output_file.write(frame)

    frame_counter += 1
    pbar.update(1)

  vs.release()

  if GET_VIDEO_OUTPUT:
    output_file.release()
    files.download(output_fname)

  # print statistics about people detected
  print("number of people detected: ", len(object_id_to_images))
  for (person_id, images) in object_id_to_images.items():
    print("person ID: ", person_id, ", ", "number of images: ", len(images))

  return object_id_to_images, object_id_to_bbox, frame_to_bbox

# important config values
MAX_FRAMES = 250            # maximum number of frames to process
STRIDE = 1                  # sample frames every # of frames
GET_VIDEO_OUTPUT = False    # if video output of detection and tracking is needed

# run detection and tracking for both videos
id_to_images_1, id_to_bbox_1, frame_to_bbox_1 = get_track_information(curr_query_video, 
                                                           max_frames=MAX_FRAMES,
                                                           stride=STRIDE, GET_VIDEO_OUTPUT=GET_VIDEO_OUTPUT)
id_to_images_2, id_to_bbox_2, frame_to_bbox_2 = get_track_information(curr_gallery_video, 
                                                           max_frames=MAX_FRAMES, 
                                                           stride=STRIDE, GET_VIDEO_OUTPUT=GET_VIDEO_OUTPUT)

# consolidate track information across videos
id_to_images_list = [id_to_images_1, id_to_images_2]
id_to_bbox_list = [id_to_bbox_1, id_to_bbox_2]
frame_to_bbox_list = [frame_to_bbox_1, frame_to_bbox_2]

'''### New Detection and Tracking [NOT USED]
NMS + tracks + filtering tracks (from Visual Contact Tracing notebook)

#### Setup and Get Frames
'''

# detectron pre-trained model setup

cpu_device = torch.device('cpu')
cfg_name = "keypoint_rcnn_R_50_FPN_3x.yaml"
DEFAULT_CONFIG = f'COCO-Keypoints/{cfg_name}'
DEFAULT_CONF_THRESH = 0.1
DEFAULT_OPTS = ['MODEL.WEIGHTS', model_zoo.get_checkpoint_url(f'COCO-Keypoints/{cfg_name}')]
# https://github.com/facebookresearch/detectron2/blob/04958b93e1232935e126c2fd9e6ccd3f57c3a8f3/detectron2/utils/visualizer.py#L32

def setup_cfg(config=DEFAULT_CONFIG, opts=DEFAULT_OPTS, conf_thresh=DEFAULT_CONF_THRESH):
  '''Set up config values for Detectron pretrained model'''

  # load config from file and arguments
  cfg = get_cfg()
  if not torch.cuda.device_count():
    print('Running on CPU')
    cfg.MODEL.DEVICE = 'cpu'
  cfg.merge_from_file(model_zoo.get_config_file(config))
  cfg.merge_from_list(opts)

  # set score_threshold for builtin models
  cfg.MODEL.RETINANET.SCORE_THRESH_TEST = conf_thresh
  cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = conf_thresh
  cfg.MODEL.PANOPTIC_FPN.COMBINE.INSTANCES_CONFIDENCE_THRESH = conf_thresh
  cfg.freeze()
  return cfg

def process_video(video_input, max_frames, stride=1):
  '''Returns frames and detectron predictions at a specified stride value.'''

  video_filename = video_input.split(".")[-1]
  vs = cv2.VideoCapture(video_input)
  width = int(vs.get(cv2.CAP_PROP_FRAME_WIDTH))
  height = int(vs.get(cv2.CAP_PROP_FRAME_HEIGHT))
  frames_per_second = vs.get(cv2.CAP_PROP_FPS)
  num_frames = int(vs.get(cv2.CAP_PROP_FRAME_COUNT))

  # set max frames to process to limit time
  if max_frames:
    max_frames = max_frames
  else:
    max_frames = num_frames

  frame_counter = 0
  pbar = tqdm(total = max_frames)

  frames = []
  all_predictions = []

  while True:
    frame = vs.read()[1]

    if frame is None or frame_counter >= max_frames:
      break
    
    if frame_counter % stride == 0:
      frames.append(frame)
      predictions = predictor(frame)
      all_predictions.append(predictions)

    frame_counter += 1
    pbar.update(1)

  vs.release()

  return frames, all_predictions

# set up config and predictor for Detectron
cfg = setup_cfg()
predictor = DefaultPredictor(cfg)

# choose input video files from list
input_video_file_1 = input_video_files[12]
input_video_file_2 = input_video_files[13]

# get frames and predictions
frames_1, all_predictions_1 = process_video(input_video_file_1, max_frames=500, stride=1)
frames_2, all_predictions_2 = process_video(input_video_file_2, max_frames=500, stride=1)

'''#### Non-Maximum Suppression (not currently used)'''

!pip install nms

# indicies = nms.nms.nms_rboxes(rrects, scores)
# boxes in all_flowed_boxes are (xA, xB, yA, yB, prob)
# we need boxes as (x, y, w, h) and scores as (prob)

from nms import nms

# copied from nms, their version prints to stdout verbosely
def rboxes(rrects, scores, nms_algorithm=nms.default_algorithm, **kwargs):
    '''
    Non Maxima Suppression for rotated rectangles

    :param rrects: a list of polygons, each described by ((cx, cy), (w,h), deg)
    :type rrects: list
    :param scores: a list of the scores associated with the rects
    :type scores: list
    :param nms_algorithm: the NMS comparison function to use, kwargs will be passed to this function. Defaults to :func:`nms.malisiewicz.NMS`
    :type nms_algorithm: function
    :returns: an array of indicies of the best rrects
    '''

    # convert the rrects to polys
    polys = []
    for rrect in rrects:
        r = cv2.boxPoints(rrect)
        polys.append(r)
    return nms.polygons(polys, scores, nms_algorithm, **kwargs)

def run_nms(all_predictions):
  all_augmented_boxes = []
  all_indices = []

  for frame_idx, predictions in enumerate(tqdm(all_predictions)):
    #'pred_boxes', 'scores', 'pred_classes', 'pred_keypoints'
    instances = predictions['instances'].to(cpu_device)
    boxes = np.asarray(instances.pred_boxes.tensor)
    boxes_scores = np.asarray(instances.scores)
    boxes_scores = np.expand_dims(boxes_scores, axis=1)
    boxes = np.append(boxes, boxes_scores, axis=1)

    rrects = []
    scores = []
    combined_boxes = boxes

    for box in combined_boxes:
      xA, yA, xB, yB, prob = box
      assert xA < xB, (xA, xB)
      assert yA < yB, (yA, yB)
      xC = (xB - xA) / 2
      yC = (yB - yA) / 2
      w = xB - xA
      h = yB - yA
      # ((cx, cy), (w,h), deg)
      rrects.append(((xA, yA), (w, h), 0))
      scores.append(prob)
    
    indices = rboxes(rrects, scores)
    assert len(indices) <= len(rrects), (len(indices), len(rrects))

    augmented_boxes = [box[:4] for i, box in enumerate(combined_boxes) if i in indices]
    all_augmented_boxes.append(augmented_boxes)
    all_indices.append(indices)

  return all_augmented_boxes, all_indices

all_augmented_boxes_1, all_indices_1 = run_nms(all_predictions_1)
all_augmented_boxes_2, all_indices_2 = run_nms(all_predictions_2)

'''#### Tracking'''

# TRACKING VARIABLES

DEFAULT_HIDE_KEYPOINTS = True
DEFAULT_HIDE_BOXES = False
SHOW_START_END_FRAMES = 0
T = 51
MAX_COST = .99

# TRACKING HELPER FUNCTIONS

# https://github.com/facebookresearch/DetectAndTrack/blob/d66734498a4331cd6fde87d8269499b8577a2842/lib/core/tracking_engine.py#L106
def compute_pairwise_iou(a, b):
  '''
  a, b (np.ndarray) of shape Nx4 and Mx4.
  The output is NxM, for each combination of boxes.
  '''

  C = 1 - bbox_overlaps(
    np.ascontiguousarray(a, dtype=np.float64),
    np.ascontiguousarray(b, dtype=np.float64),
  )
  return C


def compute_distance_matrix(prev_boxes, cur_boxes):
  return compute_pairwise_iou(prev_boxes, cur_boxes)


# https://github.com/facebookresearch/DetectAndTrack/blob/d66734498a4331cd6fde87d8269499b8577a2842/lib/core/tracking_engine.py#L184
def bipartite_matching_greedy(C, prev_tracks, next_track_id):
    '''
    Computes the bipartite matching between the rows and columns, given the
    cost matrix, C.
    '''
    C = C.copy()  # to avoid affecting the original matrix
    prev_ids = []
    cur_ids = []
    min_costs = []
    while (C == np.inf).sum() != C.size:
      # Find the lowest cost element
      min_idx = C.argmin()
      i, j = np.unravel_index(min_idx, C.shape)
      min_val = C[i][j]
      #print('min_idx:', min_idx, 'min_val:', min_val, 'i:', i, 'j:', j)

      # Add to results
      min_cost = C.min()
      min_costs.append(min_cost)
      prev_ids.append(i)
      if (not MAX_COST) or (min_cost < MAX_COST):
        cur_ids.append(j)
      else:
        cur_ids.append(-1)

      # Remove from cost matrix
      track = prev_tracks[i]
      track_idxs = [
        idx for idx in range(len(prev_tracks))
        if prev_tracks[idx] == track
      ]

      C[:, j] = np.inf
      for track_idx in track_idxs:
        C[track_idx, :] = np.inf
      #num_removed_costs = (C == np.inf).sum()

    mean_min_cost = np.mean(min_costs)
    mean_min_cost_idx = np.argmin(min_costs)
    mean_min_cost_track = cur_ids[mean_min_cost_idx]
    if mean_min_cost_track == -1:
      max_min_cost_track = next_track_id[0]
    mean_min_costs.append((mean_min_cost, mean_min_cost_track))

    max_min_cost = np.max(min_costs)
    max_min_cost_idx = np.argmax(min_costs)
    max_min_cost_track = cur_ids[max_min_cost_idx]
    if max_min_cost_track == -1:
      max_min_cost_track = next_track_id[0]
    max_min_costs.append((max_min_cost, max_min_cost_track))

    return prev_ids, cur_ids


def compute_matches(prev_boxes, cur_boxes, prev_tracks, next_track_id):
  assert len(prev_boxes) == len(prev_tracks)
  matches = -np.ones((len(cur_boxes)), dtype=np.int32)
  if not prev_boxes.size:
    return matches
  
  C = compute_distance_matrix(prev_boxes, cur_boxes)
  prev_inds, next_inds = bipartite_matching_greedy(C, prev_tracks, next_track_id)
  assert(len(prev_inds) == len(next_inds))
  for i in range(len(prev_inds)):
    matches[next_inds[i]] = prev_inds[i]
  return matches


def get_frame_tracks(matches, prev_tracks, next_track_id):
  frame_tracks = []
  for i, m in enumerate(matches):
    if m == -1 or m >= len(prev_tracks): 
      frame_tracks.append(next_track_id[0])
      next_track_id[0] += 1
    else:
      frame_tracks.append(prev_tracks[m])
  return frame_tracks


def visualize_predictions(frame, instances, hide_keypoints=DEFAULT_HIDE_KEYPOINTS, hide_boxes=DEFAULT_HIDE_BOXES):
    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)

    if hide_keypoints or hide_boxes:
      instances = instances.to(cpu_device)
    if hide_keypoints:
      instances.remove('pred_keypoints')
    if hide_boxes:
      instances.pred_boxes.tensor = torch.Tensor()
    
    # https://github.com/facebookresearch/detectron2/blob/master/detectron2/utils/video_visualizer.py#L53
    # https://github.com/facebookresearch/detectron2/blob/b6fe828a2f3b2133f24cb93c1d0d74cb59c6a15d/detectron2/utils/video_visualizer.py#L53
    vis_frame = video_visualizer.draw_instance_predictions(frame, instances)
    # Converts Matplotlib RGB format to OpenCV BGR format
    vis_frame = cv2.cvtColor(vis_frame.get_image(), cv2.COLOR_RGB2BGR)

    return vis_frame

def compute_tracks(frames, all_predictions, all_augmented_boxes=None, USE_AUGMENTED=False):
  # compute tracks, inspired by:
  # https://github.com/facebookresearch/DetectAndTrack/blob/d66734498a4331cd6fde87d8269499b8577a2842/lib/core/tracking_engine.py#L272
  all_prev_boxes = []
  video_tracks = []
  next_track_id = [0]
  for frame_id, (frame, predictions) in enumerate(tqdm(zip(frames, all_predictions), total=len(frames))):
    instances = predictions['instances'].to(cpu_device)  
    if USE_AUGMENTED:
      cur_boxes = all_augmented_boxes[frame_id]
    else:
      cur_boxes = np.asarray(instances.pred_boxes.tensor)

    prev_boxes = np.vstack(all_prev_boxes) if all_prev_boxes else np.array([])
    all_prev_tracks = video_tracks[
      max(0, frame_id - len(all_prev_boxes)) :
      max(0, frame_id)
    ]

    prev_tracks = np.hstack(all_prev_tracks) if all_prev_tracks else np.array([])

    if len(cur_boxes) == 0:
      matches = []
    else:
      matches = compute_matches(prev_boxes, cur_boxes, prev_tracks, next_track_id)
    # matches[i] contains the index of the box in the previous frames
    # corresponding to the box with index i in the current frame

    frame_tracks = get_frame_tracks(matches, prev_tracks, next_track_id)
    assert len(np.unique(frame_tracks)) == len(frame_tracks), (
        len(np.unique(frame_tracks)), len(frame_tracks)
    )
    video_tracks.append(np.array(frame_tracks))
    all_prev_boxes.append(cur_boxes)
    # if len(all_prev_boxes) > T:
    #   all_prev_boxes = all_prev_boxes[1:]

    SHOW_FRAME_ON_NEW_TRACK = False
    HAS_NEW_MATCH = any([match == -1 for match in matches])
    if frame_id < SHOW_START_END_FRAMES or frame_id >= len(frames) - SHOW_START_END_FRAMES or (
        SHOW_FRAME_ON_NEW_TRACK and HAS_NEW_MATCH
    ):
      print('Visualizing frame_id:', frame_id)
      if HAS_NEW_MATCH:
        print('New match:')
      vis_frame = visualize_predictions(frame, instances)
      for box, frame_track in zip(cur_boxes, frame_tracks):
        cv2.putText(vis_frame, str(frame_track), (int(box[0]-5), int(box[1]-5)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0))
        cv2.putText(vis_frame, str(frame_track), (int(box[0]-4), int(box[1]-4)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255))

      cv2_imshow(vis_frame)

  # TODO: filter out large position jumps that immediately return after one frame
  mean_min_costs.sort(key=lambda tup: tup[0])
  max_min_costs.sort(key=lambda tup: tup[0])

  return video_tracks, all_prev_boxes

mean_min_costs = []
max_min_costs = []
video_tracks_1, all_prev_boxes_1 = compute_tracks(frames_1, all_predictions_1)

mean_min_costs = []
max_min_costs = []
video_tracks_2, all_prev_boxes_2 = compute_tracks(frames_2, all_predictions_2)

'''#### Filtering Tracks'''

# for tracking
def setup_filtering(video_tracks, all_predictions, all_augmented_boxes=None, USE_AUGMENTED=False):
  num_tracks = 1 + max(max(frame_tracks) for frame_tracks in video_tracks)
  print('num_tracks:', num_tracks)

  boxes_by_track = {}
  for frame_idx, (predictions, frame_tracks) in enumerate(zip(all_predictions, video_tracks)):
    instances = predictions['instances'].to(cpu_device)
    if USE_AUGMENTED:
      boxes = np.asarray(all_augmented_boxes[frame_idx])
    else:
      boxes = np.asarray(instances.pred_boxes.tensor)
    
    for box, track in zip(boxes, frame_tracks):
      boxes_by_track.setdefault(track, [])
      boxes_by_track[track].append(box)

    # fill in with nans for missing tracks
    all_tracks = set(range(num_tracks))
    missing_tracks = all_tracks - set(frame_tracks)

    for missing_track in missing_tracks:
      boxes_by_track.setdefault(missing_track, [])
      boxes_by_track[missing_track].append([np.nan for _ in range(4)])

  filtered_boxes_by_track = {}
  for track, boxes in boxes_by_track.items():
    filtered_boxes = np.array(boxes)
    
    assert len(filtered_boxes) == len(boxes), (
        len(filtered_boxes), len(boxes)
    )
    filtered_boxes_by_track[track] = filtered_boxes

  return filtered_boxes_by_track

filtered_boxes_by_track_1 = setup_filtering(video_tracks_1, all_predictions_1)
filtered_boxes_by_track_2 = setup_filtering(video_tracks_2, all_predictions_2)

# at least this ratio must be missing on both sides in order to remove
REFERENCE_MIN_EMPTY_RATIO = 1
FILTER_ENDS = True
FILTER_TUPS = [
  # (a, b)
  # a: reference_radius
  # b: max_consecutive_nonempty_to_remove
  (3, 5),
  (10, 20),
  (5, 2),
]

def filter_tracks(frames, filtered_boxes_by_track):
  # filtering
  for filter_tup in FILTER_TUPS:
    print('filter_tup:', filter_tup)
    reference_radius, max_consecutive_nonempty_to_remove = filter_tup

    # for tracking
    for track, filtered_boxes in filtered_boxes_by_track.items():
      N = len(filtered_boxes)
      for i in range(N):
        start_ref_start_idx = max(0, i - reference_radius)
        start_ref_end_idx = i - 1
        mid_start_idx = i
        mid_end_idx = i + max_consecutive_nonempty_to_remove - 1
        end_ref_start_idx = mid_end_idx + 1
        end_ref_end_idx = min(end_ref_start_idx + reference_radius - 1, N - 1)

        start_ref_vals = filtered_boxes[start_ref_start_idx : start_ref_end_idx + 1]
        mid_vals = filtered_boxes[mid_start_idx : mid_end_idx + 1]
        end_ref_vals = filtered_boxes[end_ref_start_idx : end_ref_end_idx + 1]
        
        start_ref_mask = np.all(np.isnan(start_ref_vals), axis=1)
        mid_mask = np.all(np.isnan(mid_vals), axis=1)
        end_ref_mask = np.all(np.isnan(end_ref_vals), axis=1)

        num_mid_nonempty = (~mid_mask).sum()
        mid_nonempty_idxs = [i + j for j in np.where(~mid_mask)[0]]

        start_ref_empty_ratio = start_ref_mask.mean()
        end_ref_empty_ratio = end_ref_mask.mean()

        start_condition = start_ref_empty_ratio >= REFERENCE_MIN_EMPTY_RATIO
        end_condition = end_ref_empty_ratio >= REFERENCE_MIN_EMPTY_RATIO
        if FILTER_ENDS:
          start_condition = start_condition or np.isnan(start_ref_empty_ratio)
          end_condition = end_condition or np.isnan(end_ref_empty_ratio)
        mid_condition = num_mid_nonempty > 0

        if start_condition and mid_condition and end_condition:
          print('removing num_mid_nonempty:', num_mid_nonempty)
          print('mid_nonempty_idxs:', mid_nonempty_idxs)
          filtered_boxes[mid_start_idx : mid_end_idx + 1] = np.nan

  # getting consumable results
  object_id_to_images = {}
  object_id_to_bbox = {}
  frame_to_bbox = {}

  for frame_number, frame in tqdm(enumerate(frames)):
    for frame_track, boxes in filtered_boxes_by_track.items():
      # get data
      box = boxes[frame_number]

      if not np.isnan(box).any():
        box = [int(v) for v in box]
        (startX, startY, endX, endY) = box
        imCrop = frame[startY:endY, startX:endX]

        # save data
        if frame_track in object_id_to_images:
          object_id_to_images[int(frame_track)].append(imCrop)
        else:
          object_id_to_images[int(frame_track)] = [imCrop]

        if frame_track in object_id_to_bbox:
          object_id_to_bbox[int(frame_track)].append(tuple(box))
        else:
          object_id_to_bbox[int(frame_track)] = [tuple(box)]
        
        if frame_number in frame_to_bbox:
          frame_to_bbox[frame_number].append(box)
        else:
          frame_to_bbox[frame_number] = [tuple(box)]

  return object_id_to_images, object_id_to_bbox, frame_to_bbox

object_id_to_images_1, object_id_to_bbox_1, frame_to_bbox_1 = filter_tracks(frames_1, filtered_boxes_by_track_1)

object_id_to_images_2, object_id_to_bbox_2, frame_to_bbox_2 = filter_tracks(frames_2, filtered_boxes_by_track_2)

oiti_list = [object_id_to_images_1, object_id_to_images_2]
oitb_list = [object_id_to_bbox_1, object_id_to_bbox_2]
ftbb_list = [frame_to_bbox_1, frame_to_bbox_2]

oiti_list_rev = [object_id_to_images_2, object_id_to_images_1]
oitb_list_rev = [object_id_to_bbox_2, object_id_to_bbox_1]
ftbb_list_rev = [frame_to_bbox_2, frame_to_bbox_1]

'''##### Random Stuff'''

def output():  
  PRINT_ON_FRAME = True

  vs = cv2.VideoCapture(input_video_files[7])
  width = int(vs.get(cv2.CAP_PROP_FRAME_WIDTH))
  height = int(vs.get(cv2.CAP_PROP_FRAME_HEIGHT))

  out = cv2.VideoWriter('outpy.avi', cv2.VideoWriter_fourcc('M','J','P','G'), 10, (width, height))

  for frame_number in range(0, len(frames)):
    frame = frames[frame_number]
    # boxes = filtered_boxes_by_track[frame_number]
    # frame_tracks = video_tracks[frame_number]
    # print(len(boxes) == len(frame_tracks))

    if frame_number % 3 == 0:
      for track, boxes in filtered_boxes_by_track.items():
        box = boxes[frame_number]
        if not np.isnan(box).any():
          box = [int(v) for v in box]
          (startX, startY, endX, endY) = box

          if PRINT_ON_FRAME:
            cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 1)
            cv2.putText(frame, str(track), (int(box[0]-5), int(box[1]-5)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0))
            cv2.putText(frame, str(track), (int(box[0]-4), int(box[1]-4)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255))

    out.write(frame)

  out.release()

# OLD CODE
def get_data_from_tracking(frames, video_tracks, all_prev_boxes):
  PRINT_ON_FRAME = False

  object_id_to_images = {}
  object_id_to_bbox = {}
  frame_to_bbox = {}

  for frame_number in range(0, len(frames)):
    frame = frames[frame_number]
    boxes = all_prev_boxes[frame_number]
    frame_tracks = video_tracks[frame_number]

    # print(len(boxes) == len(frame_tracks))

    for box, frame_track in zip(boxes, frame_tracks):
      box = [int(v) for v in box]
      (startX, startY, endX, endY) = box
      imCrop = frame[startY:endY, startX:endX]

      if PRINT_ON_FRAME:
        cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 1)
        cv2.putText(frame, str(frame_track), (int(box[0]-5), int(box[1]-5)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0))
        cv2.putText(frame, str(frame_track), (int(box[0]-4), int(box[1]-4)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255))
        cv2_imshow(frame)

      # save data
      if frame_track in object_id_to_images:
        object_id_to_images[int(frame_track)].append(imCrop)
      else:
        object_id_to_images[int(frame_track)] = [imCrop]

      if frame_track in object_id_to_bbox:
        object_id_to_bbox[int(frame_track)].append(tuple(box))
      else:
        object_id_to_bbox[int(frame_track)] = [tuple(box)]
      
      if frame_number in frame_to_bbox:
        frame_to_bbox[frame_number].append(box)
      else:
        frame_to_bbox[frame_number] = [tuple(box)]

  return object_id_to_images, object_id_to_bbox, frame_to_bbox



# frames_7, all_predictions_7 = process_video(input_video_files, 7, stride=1)
# video_tracks_7, all_prev_boxes_7 = compute_tracks(frames_7, all_predictions_7)
# object_id_to_images_7, object_id_to_bbox_7, frame_to_bbox_7 = get_data_from_tracking(frames_7, video_tracks_7, all_prev_boxes_7)

num_ids = len(object_id_to_images_11)
for idx in range(num_ids):
  print(idx)
  print(len(object_id_to_images_11[idx]))
  cv2_imshow(object_id_to_images_11[idx][6])

object_id_to_images_11.keys()

frames_11, all_predictions_11 = process_video(input_video_files, 11, stride=2)
video_tracks_11, all_prev_boxes_11 = compute_tracks(frames_11, all_predictions_11)
object_id_to_images_11, object_id_to_bbox_11, frame_to_bbox_11 = get_data_from_tracking(frames_11, video_tracks_11, all_prev_boxes_11)

num_ids = len(object_id_to_images_11)
for idx in range(0, num_ids):
  print(len(object_id_to_images_11[idx]))
  cv2_imshow(object_id_to_images_11[idx][0])

# print statistics
# print("# frames: ", len(frames))
# print("# video tracks: ", len(video_tracks))
# print("# boxes: ", len(all_prev_boxes))
# print("# object_id_to_images: ", len(object_id_to_images))
# print("# frame_to_bbox: ", len(frame_to_bbox))

'''### Towards Realtime MOT [NOT USED]
Experiment with using Realtime MOT library (https://github.com/Zhongdao/Towards-Realtime-MOT) for e2e object detection and tracking

#### Setup
'''

!pip install folium==0.2.1
!pip install pytest==3.8
!pip install motmetrics
!pip install lap

# Commented out IPython magic to ensure Python compatibility.
MOT_DIR = "Towards-Realtime-MOT/"
MOT_DIR_FULL = "/content/drive/My Drive/Towards-Realtime-MOT/"
# %cd $MOT_DIR_FULL

'''#### Testing with default demo.py'''

path_to_input_video = "../reid-sample-videos/VIRAT_S_000002_final.mp4"
path_to_output_root = "../reid-output-videos/"
path_to_model_weights = "jde.1088x608.uncertainty.pt"
path_to_demo_script = "demo.py"

print("Input video: ", path_to_input_video)
print("Output root: ", path_to_output_root)
print("Model weights: ", path_to_model_weights)

!python $path_to_demo_script --input-video $path_to_input_video --weights $path_to_model_weights --output-format video --output-root $path_to_output_root --conf-thres 0.8

'''#### Experimental: One Video at a Time
Good for testing other approaches (ex. real time MOT)
'''

# configure input video
VID_NUMBER = 0

video_filename = input_video_files[VID_NUMBER][:-4]
video_input = input_video_files[VID_NUMBER]
vs = cv2.VideoCapture(video_input)

width = int(vs.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(vs.get(cv2.CAP_PROP_FRAME_HEIGHT))
frames_per_second = vs.get(cv2.CAP_PROP_FPS)

print(video_filename)

# configure keypoint output video
output_fname = f'{video_filename}-output.mp4'
output_file = cv2.VideoWriter(
  filename=output_fname,
  fourcc=cv2.VideoWriter_fourcc(*'mp4v'),
  fps=float(frames_per_second),
  frameSize=(width, height),
  isColor=True,
)

num_frames = int(vs.get(cv2.CAP_PROP_FRAME_COUNT))

# RUNNING KP DETECTION AND CENTROID TRACKING

# initialize centroid tracker
ct = CentroidTrackerModified()

# settings
GET_VIDEO_OUTPUT = False
frame_counter = 0
num_frames_to_output = 750
padding = 5
pbar = tqdm(total = num_frames_to_output)

# to store cropped images
object_id_to_images = {}

# read frames
while True:
  frame = vs.read()[1]

  if frame is None or frame_counter > num_frames_to_output:
    break
  
  outputs = predictor(frame)
  predictions = outputs["instances"].to("cpu")
  predictions.remove("pred_keypoints")
  tensors = predictions.pred_boxes.tensor.numpy()

  rects = []
  cropped_images = []

  for idx, tensor in enumerate(tensors):
    box = [int(v) for v in tensor]
    rects.append(box)

    # print bbox output on frame
    if GET_VIDEO_OUTPUT:
      (startX, startY, endX, endY) = box
      cv2.rectangle(frame, (startX, startY), 
                    (endX, endY), (0, 255, 0), 1)


  # update centroid tracker using bounding boxes
  (object_to_centroids, object_to_bboxes) = ct.update(rects)

  # draw over objects with information, loop over tracked objects
  for (objectID, centroid) in object_to_centroids.items():
    # draw ID and centroids of object on output frame
    if GET_VIDEO_OUTPUT:
      text = "ID {}".format(objectID)
      cv2.putText(frame, text, (centroid[0] - 10, centroid[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
      cv2.circle(frame, (centroid[0], centroid[1]), 4, (0, 255, 0), -1)

    # store images only every 25 frames
    if frame_counter % 25 == 0:
      (startX, startY, endX, endY) = box = object_to_bboxes[objectID]
      imCrop = frame[startY:endY, startX:endX]

      if objectID in object_id_to_images:
        object_id_to_images[objectID].append(imCrop)
      else:
        object_id_to_images[objectID] = [imCrop]

  if GET_VIDEO_OUTPUT:
    output_file.write(frame)

  frame_counter += 1
  pbar.update(1)

# release input video and output video services
vs.release()

if GET_VIDEO_OUTPUT:
  output_file.release()

oiti_list.append(object_id_to_images)

# statistics about processed video
print("number of people detected: ", len(object_id_to_images))

for (person_id, images) in object_id_to_images.items():
  print("person ID: ", person_id, ", ", "number of images: ", len(images))

# write to dataset
oiti_dataset = "oiti-dataset-" + str(VID_NUMBER)
if not os.path.isdir(oiti_dataset):
  os.mkdir(oiti_dataset)

for idx, (key, val) in enumerate(object_id_to_images.items()):
  os.mkdir(os.path.join(oiti_dataset, str(idx)))
  for imgidx, img in enumerate(val):
    # new_img = imutils.resize(img, width=200) # resize makes for a lot of issues
    filename = "img_" + str(imgidx) + ".jpg"
    cv2.imwrite(os.path.join(oiti_dataset, str(idx), filename), img)





'''#### Save dataset for torchreid'''

# run once for each new dataset
dataset_dir = "new_dataset"
if not os.path.isdir(dataset_dir):
  os.mkdir(dataset_dir)

SHOT = "multi_shot"
if not os.path.isdir(os.path.join(dataset_dir, SHOT)):
  os.mkdir(os.path.join(dataset_dir, SHOT))

camera_number = input_video_files.index(video_input)

def save_images_to_dataset(camera_num, object_id_to_images):
  # create camera directory
  camera_dir = "camera_" + str(camera_num)
  os.mkdir(os.path.join(dataset_dir, SHOT, camera_dir))
  os.mkdir(camera_dir)

  num_digits_to_pad = len(str(len(object_id_to_images)))

  for (person_id, images) in object_id_to_images.items():
    # create person directory
    person_dir = "person_" + str(person_id).zfill(num_digits_to_pad)
    os.mkdir(os.path.join(dataset_dir, SHOT, camera_dir, person_dir))

    pad = len(str(len(images)))
    for idx, img in enumerate(images):
      # save image to person directory
      filename = "img_" + str(idx).zfill(pad) + ".png"
      cv2.imwrite(os.path.join(dataset_dir, SHOT, camera_dir, person_dir, filename), img)

save_images_to_dataset(camera_number, object_id_to_images)

# repeat steps for all cameras in dataset

'''## Subject Re-ID
- Based on Person_reID_baseline_pytorch (prbp) library (https://github.com/layumi/Person_reID_baseline_pytorch)
- Code modified to suit CTAI purposes => can be cloned from MLDSAI repository or taken from Google Drive

Library script breakdown:
- prepare.py = prepare certain datasets (UNUSED)
- train.py = training (UNUSED)
- test.py = test only, inference + extracting features
- evaluate.py = get results (Rank@1, Rank@5, Rank@10 and mAP), plot CMC curves
- demo.py = retrieve results and display visualization

### Setup
'''

# Commented out IPython magic to ensure Python compatibility.
# set this to True if cloning from MLDSAI repository, False if from Google Drive
CLONE_PRBP = True
prbp_dir = "Person_reID_baseline_pytorch"


# download model from google drive to colab
# https://drive.google.com/file/d/1XVEYb0TN2SbBYOqf8SzazfYZlpH9CxyE/view
def download_file_from_google_drive(file_id, file_name):
  # download a file from the Google Drive link
  !rm -f ./cookie
  !curl -c ./cookie -s -L "https://drive.google.com/uc?export=download&id=$file_id" > /dev/null
  confirm_text = !awk '/download/ {print $NF}' ./cookie
  confirm_text = confirm_text[0]
  !curl -Lb ./cookie "https://drive.google.com/uc?export=download&confirm=$confirm_text&id=$file_id" -o $file_name
  with open(file_name, 'rb') as f:
    data = f.read()
    print('downloaded', len(data), 'bytes')


if CLONE_PRBP:
#   %cd "/content"

  if os.path.isdir(prbp_dir):
    prbp_dir = "/content/" + prbp_dir
#     %cd $prbp_dir
  else:
    # change credentials to access MLDSAI person re-ID repository
    username = "<your github username here>"
    password = "<your github password here>"
    repo_url = f"https://{username}:{password}@github.com/MLDSAI/Person_reID_baseline_pytorch.git"
    !git clone {repo_url}

    prbp_dir = "/content/" + prbp_dir
#     %cd $prbp_dir

    # download and unzip pretrained model
    model_zip_name = "model.zip"
    download_file_from_google_drive("1XVEYb0TN2SbBYOqf8SzazfYZlpH9CxyE", model_zip_name)
    !unzip $model_zip_name -d "model_folder"
    !mv "model_folder/model" .
    !rm -rf "model_folder"
else: 
  # use google drive
  prbp_dir = "/content/drive/My Drive/" + prbp_dir
#   %cd $prbp_dir

'''### Dataset Preparation

#### Create or retrieve person re-id dataset
'''

def save_images_to_prbp_dataset(prbp_dir, query_path, gallery_path, query_images, gallery_images):
  '''Creates and saves new dataset to disk for re-id library consumption.
  
  Structure of the new dataset is:
  new_dataset
      |___ query
            |___ 001 (pid 1)
            |___ 002 (pid 2)
            |___ ...
      |___ gallery
            |___ 001 (pid 1)
            |___ 002 (pid 2)
            |___ ...

  prbp_dir -- full path of the directory containing the prbp re-id library code
  query_path -- full path of the query folder of the new dataset
  gallery_path -- full path of the gallery folder of the new dataset
  query_images -- dictionary of person IDs to images for query camera
  gallery_images -- dictionary of person IDs to images for gallery_camera
  '''
  
  # create query directory
  for (person_id, images) in query_images.items():
    # create person directory
    person_dir = str(person_id).zfill(3)
    os.mkdir(os.path.join(prbp_dir, query_path, person_dir))

    # save images to person directory
    for idx, img in enumerate(images):
      filename = str(person_id).zfill(3) + "_img_" + str(idx).zfill(3) + ".jpg"
      cv2.imwrite(os.path.join(prbp_dir, query_path, person_dir, filename), img)

  # create gallery directory
  for (person_id, images) in gallery_images.items():
    # create person directory
    person_dir = str(person_id).zfill(3)
    os.mkdir(os.path.join(prbp_dir, gallery_path, person_dir))

    # save images to person directory
    for idx, img in enumerate(images):
      filename = str(person_id).zfill(3) + "_img_" + str(idx).zfill(3) + ".jpg"
      cv2.imwrite(os.path.join(prbp_dir, gallery_path, person_dir, filename), img)

def create_dataset(tracks_list, prbp_dir, dataset_name):
  '''Driver function for creating and saving new dataset to disk for re-id consumption.'''

  prbp_dataset_dir = os.path.join("/content", prbp_dir, "prbp_dataset" + "_" + str(dataset_name))
  query_path = os.path.join(prbp_dataset_dir, "query")
  gallery_path = os.path.join(prbp_dataset_dir, "gallery")
  
  # create dataset if not created yet
  if not os.path.isdir(prbp_dataset_dir):
    os.mkdir(prbp_dataset_dir)
    os.mkdir(query_path)
    os.mkdir(gallery_path)

    save_images_to_prbp_dataset(prbp_dir, query_path, gallery_path, tracks_list[0], tracks_list[1])
  
  return (prbp_dataset_dir, query_path, gallery_path)

# True if creating a new dataset, False if retrieving an existing dataset
CREATE_NEW_DATASET = True

# if creating a new dataset: CHANGE THIS VARIABLE to create a new dataset (any name works)
# if retrieving an existing dataset: CHANGE THIS VARIABLE to the name of the dataset you want to retrieve
# ex. DATASET_DIR_NAME = "demo_500" for sample data in repo
DATASET_DIR_NAME = f"demo_{MAX_FRAMES}"

if CREATE_NEW_DATASET: # create new dataset
  # save video information to files
  id_to_bbox_file = f"{DATASET_DIR_NAME}_id_to_bbox.npy"
  frame_to_bbox_file = f"{DATASET_DIR_NAME}_frame_to_bbox.npy"

  with open(id_to_bbox_file, "wb") as f:
    np.save(f, np.array(id_to_bbox_list))

  with open(frame_to_bbox_file, "wb") as f:
    np.save(f, np.array(frame_to_bbox_list))

  # create and save dataset
  (prbp_dataset_dir, query_path, gallery_path) = create_dataset(id_to_images_list, prbp_dir, DATASET_DIR_NAME)

  # get test dataset name in the form of (prbp_dataset_[NEW_DATASET_NAME])
  test_dataset = prbp_dataset_dir.split("/")[-1]
else: # retrieve existing dataset
  prbp_dataset_dir = os.path.join("/content", prbp_dir, "prbp_dataset" + "_" + str(DATASET_DIR_NAME))
  test_dataset = "prbp_dataset_" + str(DATASET_DIR_NAME)
  query_path = os.path.join(prbp_dataset_dir, "query")
  gallery_path = os.path.join(prbp_dataset_dir, "gallery")

  # get video information file names
  id_to_bbox_file = f"{DATASET_DIR_NAME}_id_to_bbox.npy"
  frame_to_bbox_file = f"{DATASET_DIR_NAME}_frame_to_bbox.npy"

'''### Model + Test + Demo'''

# conduct inference
# this will take a while to complete for larger datasets
# note: evaluation script call in test.py can be removed or commented out - rank and mAP scores will not be correct due to lack of ground truth

!python test.py --gpu_ids 0 --name ft_ResNet50 --test_dir $test_dataset --batchsize 32

# EXAMPLE: get and output results for a single query image in test_dataset
QUERY_NUMBER = 0
!python demo.py --query_index $QUERY_NUMBER --test_dir $test_dataset

'''#### Modified demo.py to get aggregated re-id information'''

# modified demo.py (from https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/demo.py)

# imports
import argparse
import scipy.io
import torch
import numpy as np
import os
from torchvision import datasets
import matplotlib
matplotlib.use('agg')
import matplotlib.pyplot as plt

#####################################################################
# show result
def imshow(path, title=None):
  '''Imshow for Tensor.'''
  im = plt.imread(path)
  plt.imshow(im)
  if title is not None:
      plt.title(title)
  plt.pause(0.001)  # pause a bit so that plots are updated

#######################################################################
def sort_img(qf, ql, qc, gf, gl, gc, image_datasets):
  '''Sort gallery images based on score.'''
  query = qf.view(-1,1)
  score = torch.mm(gf,query)
  score = score.squeeze(1).cpu()
  score = score.numpy()
  # predict index
  index = np.argsort(score)  # from small to large
  index = index[::-1]
  # index = index[0:2000]
  # good index
  query_index = np.argwhere(gl==ql)
  # same camera
  camera_index = np.argwhere(gc==qc)

  # good_index = np.setdiff1d(query_index, camera_index, assume_unique=True)
  junk_index1 = np.argwhere(gl==-1)
  junk_index2 = np.intersect1d(query_index, camera_index)
  junk_index = np.append(junk_index2, junk_index1) 

  mask = np.in1d(index, junk_index, invert=True)
  index = index[mask]
  
  return (index, sorted(score, reverse=True))

########################################################################
# visualize the rank result and returns the top images
def visualize_result(image_datasets, query_index, query_label, index, gallery_label, 
                     num_top_images=10, PRINT_OUTPUT=False, SAVE_IMAGE=False):
  '''Return top images and produce visualization of query image with top matching gallery images.'''
  query_path, _ = image_datasets['query'].imgs[query_index]
  query_label = query_label[query_index]

  if PRINT_OUTPUT:
    print(query_path)
    print(f'Top {num_top_images} images are as follow:')

  top_images = []
  try: 
      # Visualize Ranking Result - Graphical User Interface is needed
      if SAVE_IMAGE:
        fig = plt.figure(figsize=(16,4))
        ax = plt.subplot(1,11,1)
        ax.axis('off')
        imshow(query_path,'query')
      for i in range(num_top_images):
          img_path, _ = image_datasets['gallery'].imgs[index[i]]
          label = gallery_label[index[i]]

          if SAVE_IMAGE:
            ax = plt.subplot(1,11,i+2)
            ax.axis('off')
            imshow(img_path)
            if label == query_label:
                ax.set_title('%d'%(i+1), color='green')
            else:
                ax.set_title('%d'%(i+1), color='red')
          
          if PRINT_OUTPUT:
            print(img_path)

          top_images.append(img_path)

  except RuntimeError:
      for i in range(10):
          img_path = image_datasets.imgs[index[i]]
          if PRINT_OUTPUT:
            print(img_path[0])
          top_images.append(img_path)
      print('If you want to see the visualization of the ranking result, graphical user interface is needed.')

  # saves query image and top 10 gallery images to a png file
  fig.savefig("result.png")
  return (query_path, top_images)

'''###### New Query to Gallery Matching (Maximum Bipartite Matching)'''

def get_all_ids(pytorch_result_filename):
  '''Returns all unique query and gallery IDs.'''
  result = scipy.io.loadmat(pytorch_result_filename)
  query_label = result['query_label'][0]
  gallery_label = result['gallery_label'][0]
  qpids = np.unique(np.array(query_label))
  gpids = np.unique(np.array(gallery_label))
  return qpids, gpids

def compute_gallery_probs(image_datasets, gallery_label, index, scores, score_threshold=0.5):
  '''
  Returns numpy array of aggregated scores for all gallery IDs for a given query image,
  assuming the scores are above given score threshold.
  
  image_datasets -- torchvision image datasets created from dataset on disk
  gallery_label -- numpy array of integer person ID labels corresponding to each gallery image
  index -- numpy array of gallery image indices, sorted in order of best match to query image
  scores -- numpy array of gallery image scores, sorted in order of best score to query image
  score_threshold -- minimum score floor for being included in aggregated gallery results
  '''

  gpids = np.unique(np.array(gallery_label))
  id_to_prob = { gpid: 0 for gpid in gpids }

  for i, (idx, score) in enumerate(zip(index, scores)):
    img_path, _ = image_datasets['gallery'].imgs[idx]
    label = gallery_label[idx]
    score = scores[i]

    if score < score_threshold:
      break

    id_to_prob[label] += score

  return np.array(list(id_to_prob.values()))

# https://github.com/facebookresearch/DetectAndTrack/blob/d66734498a4331cd6fde87d8269499b8577a2842/lib/core/tracking_engine.py#L184
def bipartite_matching_greedy(C):
    '''
    Computes the bipartite matching between the rows and columns given a
    cost matrix, C.
    '''
    C = C.copy()  # to avoid affecting the original matrix
    prev_ids = []
    cur_ids = []
    min_costs = []
    while (C == np.inf).sum() != C.size:
      # find the lowest cost element
      min_idx = C.argmin()
      i, j = np.unravel_index(min_idx, C.shape)
      min_val = C[i][j]

      # add to results
      min_cost = C.min()
      min_costs.append(min_cost)
      prev_ids.append(i)
      cur_ids.append(j)
      
      # remove from cost matrix
      C[:, j] = np.inf
      C[i, :] = np.inf

    return prev_ids, cur_ids

def match_query_to_gallery(test_dataset, pytorch_result_filename, query_path):
  '''
  Returns re-ID bipartite matching in the form of a dictionary of query IDs to gallery IDs.
  
  test_dataset -- full name of test dataset in the form of (prbp_dataset_[DATASET_DIR_NAME])
  pytorch_result_filename -- filename of pytorch results from running inference on dataset
  query_path -- path to query folder in dataset
  '''

  result = scipy.io.loadmat(pytorch_result_filename)
  qpaths = result['qpaths']
  gpaths = result['gpaths']
  qpaths_to_qid = dict(zip(qpaths, range(0, len(qpaths))))
  gpaths_to_gid = dict(zip(gpaths, range(0, len(gpaths))))

  image_datasets = {x: datasets.ImageFolder(os.path.join(test_dataset, x)) for x in ['gallery','query']}

  query_feature = torch.FloatTensor(result['query_f'])
  query_cam = result['query_cam']
  query_label = result['query_label'][0]
  query_feature = query_feature.cuda()

  gallery_feature = torch.FloatTensor(result['gallery_f'])
  gallery_cam = result['gallery_cam'][0]
  gallery_label = result['gallery_label'][0]
  gallery_feature = gallery_feature.cuda()

  qpids = np.unique(np.array(query_label))
  gpids = np.unique(np.array(gallery_label))

  all_combined_weights = []

  for pid in sorted(os.listdir(query_path)):
    combined_weights = np.zeros(len(gpids))

    for img in os.listdir(os.path.join(query_path, pid)):
      qpath = os.path.join(test_dataset, "query", pid, img)

      # get sorted results
      query_index = qpaths_to_qid[qpath]
      (index, scores) = sort_img(query_feature[query_index], query_label[query_index], query_cam[query_index], 
                                 gallery_feature, gallery_label, gallery_cam, image_datasets)
      
      # aggregate gallery weights
      weights = compute_gallery_probs(image_datasets, gallery_label, index, scores, score_threshold=SCORE_THRESHOLD)
      combined_weights += weights

    all_combined_weights.append(combined_weights)

  # run greedy bipartite matching using combined weights
  all_combined_weights = np.array(all_combined_weights)
  assigned_qpids, assigned_gpids = bipartite_matching_greedy(1 / all_combined_weights) # invert matrix values
  query_to_gallery = dict(zip(assigned_qpids, assigned_gpids))

  return query_to_gallery

# adjust score_threshold if needed based on how lenient the re-ID matching should be
SCORE_THRESHOLD = 0.5

pytorch_result_filename = f'pytorch_result_{test_dataset}.mat'
all_qpids, all_gpids = get_all_ids(pytorch_result_filename)
query_to_gallery = match_query_to_gallery(test_dataset, pytorch_result_filename, query_path)
gallery_to_query = { v: k for k, v in query_to_gallery.items() }

print("Query to gallery matching: ", query_to_gallery)

'''###### Assign colors and new IDs'''

# 16 colors, accessible for 99% of the population
# https://sashamaps.net/docs/tools/20-colors/

MAX_COLORS = None

class ColorPicker():
  '''A class to retrieve unique colors from a list of accessible colors.'''
  def __init__(self):
    self.colors = [(230, 25, 75), (60, 180, 75), (255, 225, 25), (0, 130, 200), 
                   (245, 130, 48), (70, 240, 240), (240, 50, 230), (250, 190, 212), 
                   (0, 128, 128), (220, 190, 255), (170, 110, 40), (255, 250, 200), 
                   (128, 0, 0), (170, 255, 195), (0, 0, 128), (128, 128, 128)]
    self.curr_max_index = len(self.colors) - 1
    np.random.seed(42)
    
  def get_new_color(self):
    '''Returns an unseen color from the 16 accessible colors listed above in (R, G, B) tuple format.'''
    if self.curr_max_index < 0:
      raise Exception('No more colors to choose from.')

    if self.curr_max_index == 0:
      self.curr_max_index -= 1
      return self.colors[0]

    idx = np.random.randint(self.curr_max_index)
    color = self.colors[idx]

    self.colors[idx] = self.colors[self.curr_max_index]
    self.colors[self.curr_max_index] = color
    self.curr_max_index -= 1

    return color

def get_random_color():
  '''Generates a random color in (R, G, B) tuple format.'''
  color = list(np.random.choice(range(256), size=3))
  color = tuple([int(val) for val in color])
  return color

def assign_id_colors(query_to_gallery):
  '''Assign ID colors given a bipartite matching from query IDs to gallery IDs.'''
  cp = ColorPicker()

  seen_gpid_colors = {}
  query_colors = {}
  gallery_colors = {}

  for qpid, gpid in query_to_gallery.items():
    qpid_color = cp.get_new_color()

    if gpid in seen_gpid_colors: # if there is a match already seen
      query_colors[qpid] = seen_gpid_colors[gpid]
    else:
      query_colors[qpid] = qpid_color
      gallery_colors[gpid] = qpid_color
      seen_gpid_colors[gpid] = qpid_color

  return query_colors, gallery_colors

def reassign_to_new_ids(query_to_gallery):
  '''
  Return new matching IDs for original query IDs and gallery IDs across camera views.
  '''
  curr_id = 0
  new_qpids = {}
  new_gpids = {}

  # handle query IDs, including duplicates
  for qpid, gpid in query_to_gallery.items():
    if gpid == None: # unique query ID
      unique_qpids.append(qpid)
    else: # new query ID
      new_qpids[qpid] = curr_id
      new_gpids[gpid] = curr_id
      curr_id += 1

  return new_qpids, new_gpids

query_colors, gallery_colors = assign_id_colors(query_to_gallery)
new_qpids, new_gpids = reassign_to_new_ids(query_to_gallery)

print("Query ID colors: ", query_colors)
print("Gallery ID colors: ", gallery_colors)
print("New query IDs: ", new_qpids)
print("New gallery IDs: ", new_gpids)

'''#### Generate Demo Video'''

# specific videos only if using the sample videos
# from https://github.com/MLDSAI/Person_reID_baseline_pytorch.git

query_video_file = curr_query_video if "curr_query_video" in locals() else "VIRAT_S_000101_final.mp4"
gallery_video_file = curr_gallery_video if "curr_gallery_video" in locals() else "VIRAT_S_000002_final.mp4"
print("Query video: ", query_video_file)
print("Gallery video: ", gallery_video_file)

query_video_name = query_video_file.split(".")[0]
gallery_video_name = gallery_video_file.split(".")[0]

# configure query video input
assert os.path.isfile(query_video_file)
qvideo = cv2.VideoCapture(query_video_file)
qwidth = int(qvideo.get(cv2.CAP_PROP_FRAME_WIDTH))
qheight = int(qvideo.get(cv2.CAP_PROP_FRAME_HEIGHT))
qframes_per_second = qvideo.get(cv2.CAP_PROP_FPS)
qnum_frames = int(qvideo.get(cv2.CAP_PROP_FRAME_COUNT))

query_output_fname = f'{query_video_name}_output.mp4'
query_output_file = cv2.VideoWriter(
  filename=query_output_fname,
  fourcc=cv2.VideoWriter_fourcc(*'mp4v'),
  fps=float(qframes_per_second),
  frameSize=(qwidth, qheight),
  isColor=True,
)

# configure gallery video input
assert os.path.isfile(gallery_video_file)
gvideo = cv2.VideoCapture(gallery_video_file)
gwidth = int(gvideo.get(cv2.CAP_PROP_FRAME_WIDTH))
gheight = int(gvideo.get(cv2.CAP_PROP_FRAME_HEIGHT))
gframes_per_second = gvideo.get(cv2.CAP_PROP_FPS)
gnum_frames = int(gvideo.get(cv2.CAP_PROP_FRAME_COUNT))

gallery_output_fname = f'{gallery_video_name}_output.mp4'
gallery_output_file = cv2.VideoWriter(
  filename=gallery_output_fname,
  fourcc=cv2.VideoWriter_fourcc(*'mp4v'),
  fps=float(gframes_per_second),
  frameSize=(gwidth, gheight),
  isColor=True,
)

def invert_oitb(oitb):
  '''Returns bounding box to person ID mapping for use in creating demo video.'''
  inverted_oitb = {}
  for pid, bboxes in oitb.items():
    for bbox in bboxes:
      inverted_oitb[tuple(bbox)] = pid
  return inverted_oitb

# get information from files
with open(id_to_bbox_file, "rb") as f:
  id_to_bbox_loaded = np.load(f, allow_pickle=True)
  query_oitb, gallery_oitb = id_to_bbox_loaded
  query_bbox_to_id = invert_oitb(query_oitb)
  gallery_bbox_to_id = invert_oitb(gallery_oitb)

with open(frame_to_bbox_file, "rb") as f:
  frame_to_bbox_loaded = np.load(f, allow_pickle=True)
  query_frame_to_bbox, gallery_frame_to_bbox = frame_to_bbox_loaded

# set variables for printing multi-line legend on final re-ID video
TEXT_SIZE, _ = cv2.getTextSize("sample text", cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
LINE_HEIGHT = TEXT_SIZE[1] + 5
SHOW_LEGEND = False
SHOW_UNMATCHED_IDS = False

def create_output_video(video, output, frame_to_bbox, bbox_to_id, id_dict, 
                        all_pids, new_pids, colors, num_frames_to_output=1000):
  '''Produce output video with color-coded tracks based on re-identification results.'''

  frame_counter = 0
  pbar = tqdm(total = num_frames_to_output) 

  if SHOW_LEGEND:
    legend_text = "THIS ID, OTHER ID" + "\n"
    for pid1, pid2 in sorted(id_dict.items()):
      legend_text += f"ID: {pid1}, "
      if pid2:
        legend_text += f"Other ID: {pid2}" + "\n"
      else:
        legend_text += "No Match" + "\n"
    
    legend_text_lines = legend_text.split("\n")

  while True:
    frame = video.read()[1]

    if frame is None or frame_counter > num_frames_to_output:
      break

    # draw legend in corner of frame
    if SHOW_LEGEND:
      curr_y = 40
      for line in legend_text_lines:
        cv2.putText(frame, line, (10, curr_y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)
        cv2.putText(frame, line, (11, curr_y+1), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        curr_y += LINE_HEIGHT

    # draw bounding boxes around people at the given stride
    if frame_counter in frame_to_bbox:
      bboxes = frame_to_bbox[frame_counter]

      for idx, bbox in enumerate(bboxes):
        (startX, startY, endX, endY) = bbox
        pid = bbox_to_id[tuple(bbox)]

        if pid in colors: # assigned a color
          color = colors[pid]
          cv2.rectangle(frame, (startX, startY), (endX, endY), 
                        (color[2], color[1], color[0]), 2)
          
          text = f"ID: {new_pids[pid]} "
          cv2.putText(frame, text, (startX - 11, startY - 11), 
                      cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
          cv2.putText(frame, text, (startX - 10, startY - 10), 
                      cv2.FONT_HERSHEY_SIMPLEX, 0.6, (color[2], color[1], color[0]), 2)
        else:
          if SHOW_UNMATCHED_IDS:
            text = "Unmatched ID"
            cv2.rectangle(frame, (startX, startY), (endX, endY), 
                          (128, 128, 128), 2)
            cv2.putText(frame, text, (startX - 11, startY - 11), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            cv2.putText(frame, text, (startX - 10, startY - 10), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)

    output.write(frame)
    frame_counter += 1
    pbar.update(1)

  video.release()
  output.release()

# number of frames to include in the re-ID output video
# change this depending on how many frames were processed when the input videos were processed
num_frames_to_output = MAX_FRAMES if "MAX_FRAMES" in locals() else 1000
print("Maximum number of frames processed: ", MAX_FRAMES)

# get output videos
create_output_video(qvideo, query_output_file, query_frame_to_bbox, query_bbox_to_id, 
                    query_to_gallery, all_qpids, new_qpids, query_colors, num_frames_to_output)

create_output_video(gvideo, gallery_output_file, gallery_frame_to_bbox, gallery_bbox_to_id, 
                    gallery_to_query, all_gpids, new_gpids, gallery_colors, num_frames_to_output)

# download output videos
files.download(query_output_fname)
files.download(gallery_output_fname)

'''### TO DO

1. Improve object detection and tracking
2. Discuss intra-camera re-id techniques
3. Make additional visualization improvements
4. Build in human input (via ourselves or Amazon Mechanical Turk)
'''
